{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpgdusESr1nUEnWNntMX0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeyoM/Time_Series_Forecast_ML/blob/main/Forecasting_with_Recurrent_Neural_Networks_Theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "RNN is a neural network which contains recurrent layers, and a recurrent layer is a layer that can sequentially process a sequence of inputs.\n",
        "\n",
        "This inputs can vary from sentences, to in this case, time series.\n",
        "\n",
        "In the next image, we have a RNN with two recurrent layers and a final Dense Layer (output layer for this example).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1BRQtHRU67GpiW-9vIpEZMXzxN9w_o0te)\n",
        "\n",
        "As before, the input for this RNN can be a widow size, but when working with RNN our inputs should be shaped as three-dimensional ones.\n",
        "\n",
        "The first dimension is the batch size, the second dimension is the time steps, and the final dimension represents the dimensionality of the inputs at each time step. For example, in univariate time series, this last dimension is one.\n",
        "\n",
        "### How does each layer works?\n",
        "\n",
        "A recurrent layer is composed of a single memory cell which is used repeatedly to compute the outputs. This memory cell can be a simple dense layer or a complex memory cell like an LSTM. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1kwOW9CrOsISN-5ZrRLohpZHRO_PGIYl8)\n",
        "\n",
        "*** In the diagram it appears like there are multiple cells, but no, it is only one cell but it is reused multiple times by the layer.\n",
        "\n",
        "At each time step, the memory cell takes the value of the input sequence at that time step (x0 ... yn) and produces the output at that time step (y0 ... yn).\n",
        "\n",
        "And the memory cell also produces another output at each tie step called a state/context vector (H0 ... Hn). \n",
        "\n",
        "This context vector is fed as an additional input to the memory cell at the next time step. \n",
        "\n",
        "This is why it is called recurrent, part of the output at each time step, is also part of the input for the same memory cell but for the next time step.\n",
        "\n",
        "It is like when reading, as we read a sentece, we are processing each word at a time, but we also have the context of the sentence from the previous words, this context is updated all the time while reading.\n",
        "\n",
        "### Data flow on a Recurent Layer\n",
        "\n",
        "As mentioned before, the inputs for a recurrent layer shpuld be 3 dimensional arrays. For this example we are feeding a batch of four windows from a time series, each with 30 time steps, and since it is univariate it only has one value per time step. Therefore, the shape is 4x30x1.\n",
        "\n",
        "At each time step the memory cell takes a 4x1 matrix as input, as well as the state matrix from the previous time step.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=19ipYnxodBjpZ0LYQKgKQ9HJYR0bTF6Zu)\n",
        "\n",
        "The memory cell is composed of a fixed number of neurons, this neurons are also called units, in this example, the memory cell has 3 units.\n",
        "\n",
        "THerefore, the output of each time step will then be 4x3 (batch sixe x number of units).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1YvNHaXLd5FbYbKiZn8sKFinRj_QZLxq-)\n",
        "\n",
        "And the full ouptut of the layer is three dimensional. The first dimension is the batch size (4), the second one is the number of steps (30), and the last dimension is the output dimensionality, equal to the number of units (3).\n",
        "\n",
        "This output contains four multivariate time series that can be fed into another recurrent layer.\n",
        "\n",
        "In normal recurrent layers the state matrx is just a copy of the output matrix. For example H0 is just a copy of Y0\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1SPDqTctWQXvFLu7vLdcRIORbDH7cSSFg)\n",
        "\n",
        "This way, at each time step we can know what what the previous output was. In some cases we will want to output a single vector for each instance in the batch. This is called a sequence to vector RNN. What we would do is just ignore the outputs and just take the last one.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1cnXmreV-58D3YXGdM551HptC5IKQdpD5)\n",
        "\n",
        "\n",
        "## Back-Propagation Through Time (BPTT)\n",
        "\n",
        "RNN can be in some cases really tricky to train, this is because it is the equivalent to train a fairly deep neural network with one layer per time step.\n",
        "\n",
        "During training, once the loss has been computed back-propagation computes the gradients of the loss to every trainable parameter in the neural network, and to do so, it propagates the gradients backwards through the RNN. While doing this, the gradients tend to vanish or even explode. This process can be really slow.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oxB3Oi2C5zi9FoAhgf1EGW_mSNcCaK5d)\n",
        "\n",
        "One approach is to train the RNN to make a prediction at each time step. This now would be a sequence to sequence RNN instead of a sequence to vector one.\n",
        "\n",
        "The labels are equal to the inputs but just shiffted by one time step to the future.\n",
        "\n",
        "The advantage of this approach is that it proveides much more gradients for training. If the forecast is bad at one time step, there will be a string loss at that time step and it will backpropagate directly to update the weights at that time step. This will speed up the training but we will only focus at the last output when we make predictions.\n",
        "\n",
        "[](https://drive.google.com/uc?export=view&id=1HIHbeNmLgEj7W1_3cbodIXR1sJINsPfe)"
      ],
      "metadata": {
        "id": "PG4OwzAlLyr-"
      }
    }
  ]
}